{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fb183f",
   "metadata": {},
   "source": [
    "# GitLab Java Repository Scraping\n",
    "\n",
    "This notebook performs comprehensive scraping of Java repositories from GitLab, similar to the GitHub scraping functionality. It includes:\n",
    "\n",
    "1. **Repository Search**: Finding Java projects using GitLab API\n",
    "2. **Detailed Analysis**: Extracting build files, dependencies, and project structure\n",
    "3. **Dependency Extraction**: Parsing Maven and Gradle dependencies\n",
    "4. **Repository Cloning**: Downloading and analyzing repository contents\n",
    "5. **Enhanced Data Collection**: Getting contributors, issues, and commits\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have a GitLab token set in your `.env` file:\n",
    "```\n",
    "GITLAB_TOKEN=your_gitlab_token_here\n",
    "```\n",
    "\n",
    "## Output Files\n",
    "\n",
    "The notebook generates several JSON files:\n",
    "- `gitlab_java_repos.json` - Raw repository data\n",
    "- `gitlab_analyzed_repos.json` - Analyzed repositories with build files\n",
    "- `gitlab_dependencies.json` - All dependencies found\n",
    "- `gitlab_unique_dependencies.json` - Unique dependencies list\n",
    "- `gitlab_detailed_analysis.json` - Detailed file analysis\n",
    "- `gitlab_enhanced_analysis.json` - Complete data with contributors and issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eadaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# GitLab API configuration\n",
    "GITLAB_TOKEN = os.getenv(\"GITLAB_TOKEN\")  # Add this to your .env file\n",
    "GITLAB_API_URL = \"https://gitlab.com/api/v4\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {GITLAB_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(\"GitLab API configured successfully\")\n",
    "print(f\"Using GitLab API URL: {GITLAB_API_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Java repositories on GitLab\n",
    "def search_gitlab_repositories(language=\"Java\", per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Search for repositories by language on GitLab\n",
    "    \"\"\"\n",
    "    all_projects = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        \n",
    "        # GitLab API endpoint for searching projects\n",
    "        url = f\"{GITLAB_API_URL}/projects\"\n",
    "        \n",
    "        params = {\n",
    "            'search': '',  # Empty search to get all projects\n",
    "            'language': language,\n",
    "            'order_by': 'stars',\n",
    "            'sort': 'desc',\n",
    "            'per_page': per_page,\n",
    "            'page': page,\n",
    "            'visibility': 'public',\n",
    "            'simple': 'false'  # Get full project details\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                projects = response.json()\n",
    "                \n",
    "                if not projects:  # Empty response means no more pages\n",
    "                    print(f\"No more projects found on page {page}\")\n",
    "                    break\n",
    "                \n",
    "                all_projects.extend(projects)\n",
    "                print(f\"Found {len(projects)} projects on page {page}. Total: {len(all_projects)}\")\n",
    "                \n",
    "                # Rate limiting - GitLab has rate limits\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            elif response.status_code == 401:\n",
    "                print(\"Authentication failed. Check your GitLab token.\")\n",
    "                break\n",
    "            elif response.status_code == 403:\n",
    "                print(\"Rate limit exceeded. Waiting...\")\n",
    "                time.sleep(60)  # Wait 1 minute\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_projects\n",
    "\n",
    "# Get Java repositories from GitLab\n",
    "gitlab_java_repos = search_gitlab_repositories(language=\"Java\", per_page=100, max_pages=5)\n",
    "\n",
    "print(f\"\\nTotal GitLab Java repositories found: {len(gitlab_java_repos)}\")\n",
    "\n",
    "# Save the repositories to a JSON file\n",
    "with open(\"gitlab_java_repos.json\", \"w\") as f:\n",
    "    json.dump(gitlab_java_repos, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(gitlab_java_repos)} GitLab repositories to gitlab_java_repos.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20878d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed project information\n",
    "def get_gitlab_project_details(project_id):\n",
    "    \"\"\"\n",
    "    Get detailed information about a specific GitLab project\n",
    "    \"\"\"\n",
    "    url = f\"{GITLAB_API_URL}/projects/{project_id}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error getting project {project_id}: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception getting project {project_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_gitlab_project_files(project_id, ref='main', path=''):\n",
    "    \"\"\"\n",
    "    Get files from a GitLab project repository\n",
    "    \"\"\"\n",
    "    url = f\"{GITLAB_API_URL}/projects/{project_id}/repository/tree\"\n",
    "    \n",
    "    params = {\n",
    "        'ref': ref,\n",
    "        'path': path,\n",
    "        'recursive': True,\n",
    "        'per_page': 100\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error getting files for project {project_id}: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception getting files for project {project_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_gitlab_file_content(project_id, file_path, ref='main'):\n",
    "    \"\"\"\n",
    "    Get the content of a specific file from GitLab\n",
    "    \"\"\"\n",
    "    # URL encode the file path\n",
    "    import urllib.parse\n",
    "    encoded_path = urllib.parse.quote(file_path, safe='')\n",
    "    \n",
    "    url = f\"{GITLAB_API_URL}/projects/{project_id}/repository/files/{encoded_path}/raw\"\n",
    "    \n",
    "    params = {\n",
    "        'ref': ref\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Error getting file {file_path} from project {project_id}: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception getting file {file_path} from project {project_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze first few repositories to get detailed data\n",
    "analyzed_repos = []\n",
    "\n",
    "for i, repo in enumerate(gitlab_java_repos[:10]):  # Start with first 10\n",
    "    print(f\"\\nAnalyzing repository {i+1}/{min(10, len(gitlab_java_repos))}: {repo['name']}\")\n",
    "    \n",
    "    project_id = repo['id']\n",
    "    \n",
    "    # Get project files\n",
    "    files = get_gitlab_project_files(project_id)\n",
    "    \n",
    "    # Look for build files (pom.xml, build.gradle)\n",
    "    build_files = []\n",
    "    java_files = []\n",
    "    \n",
    "    for file_info in files:\n",
    "        if file_info['type'] == 'blob':  # Regular file\n",
    "            file_name = file_info['name']\n",
    "            file_path = file_info['path']\n",
    "            \n",
    "            if file_name in ['pom.xml', 'build.gradle', 'build.gradle.kts']:\n",
    "                build_files.append(file_path)\n",
    "            elif file_name.endswith('.java'):\n",
    "                java_files.append(file_path)\n",
    "    \n",
    "    # Get content of build files\n",
    "    build_file_contents = {}\n",
    "    for build_file in build_files[:3]:  # Limit to first 3 build files\n",
    "        content = get_gitlab_file_content(project_id, build_file)\n",
    "        if content:\n",
    "            build_file_contents[build_file] = content\n",
    "    \n",
    "    # Store analyzed data\n",
    "    repo_data = {\n",
    "        'id': repo['id'],\n",
    "        'name': repo['name'],\n",
    "        'path_with_namespace': repo['path_with_namespace'],\n",
    "        'description': repo.get('description', ''),\n",
    "        'web_url': repo['web_url'],\n",
    "        'star_count': repo.get('star_count', 0),\n",
    "        'forks_count': repo.get('forks_count', 0),\n",
    "        'created_at': repo.get('created_at', ''),\n",
    "        'last_activity_at': repo.get('last_activity_at', ''),\n",
    "        'default_branch': repo.get('default_branch', 'main'),\n",
    "        'build_files': build_files,\n",
    "        'java_files_count': len(java_files),\n",
    "        'build_file_contents': build_file_contents\n",
    "    }\n",
    "    \n",
    "    analyzed_repos.append(repo_data)\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nAnalyzed {len(analyzed_repos)} repositories\")\n",
    "\n",
    "# Save analyzed data\n",
    "with open(\"gitlab_analyzed_repos.json\", \"w\") as f:\n",
    "    json.dump(analyzed_repos, f, indent=2)\n",
    "\n",
    "print(f\"Saved analyzed data to gitlab_analyzed_repos.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77695c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dependencies from build files\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_maven_dependencies_gitlab(pom_content):\n",
    "    \"\"\"Extract dependencies from pom.xml content\"\"\"\n",
    "    dependencies = []\n",
    "    try:\n",
    "        root = ET.fromstring(pom_content)\n",
    "        # Handle namespaces\n",
    "        ns = {'maven': 'http://maven.apache.org/POM/4.0.0'}\n",
    "        \n",
    "        # Find dependencies\n",
    "        deps = root.findall('.//maven:dependency', ns) or root.findall('.//dependency')\n",
    "        \n",
    "        for dep in deps:\n",
    "            group_id = dep.find('maven:groupId', ns) or dep.find('groupId')\n",
    "            artifact_id = dep.find('maven:artifactId', ns) or dep.find('artifactId')\n",
    "            version = dep.find('maven:version', ns) or dep.find('version')\n",
    "            \n",
    "            if group_id is not None and artifact_id is not None:\n",
    "                dependencies.append({\n",
    "                    'groupId': group_id.text,\n",
    "                    'artifactId': artifact_id.text,\n",
    "                    'version': version.text if version is not None else 'unknown'\n",
    "                })\n",
    "    except ET.ParseError:\n",
    "        pass\n",
    "    return dependencies\n",
    "\n",
    "def extract_gradle_dependencies_gitlab(gradle_content):\n",
    "    \"\"\"Extract dependencies from build.gradle content\"\"\"\n",
    "    dependencies = []\n",
    "    # Pattern to match various gradle dependency formats\n",
    "    patterns = [\n",
    "        r\"implementation\\s+['\\\"]([^:]+):([^:]+):([^'\\\"]+)['\\\"]\",\n",
    "        r\"compile\\s+['\\\"]([^:]+):([^:]+):([^'\\\"]+)['\\\"]\",\n",
    "        r\"api\\s+['\\\"]([^:]+):([^:]+):([^'\\\"]+)['\\\"]\",\n",
    "        r\"testImplementation\\s+['\\\"]([^:]+):([^:]+):([^'\\\"]+)['\\\"]\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, gradle_content)\n",
    "        for match in matches:\n",
    "            dependencies.append({\n",
    "                'groupId': match[0],\n",
    "                'artifactId': match[1],\n",
    "                'version': match[2]\n",
    "            })\n",
    "    return dependencies\n",
    "\n",
    "# Extract dependencies from all analyzed repositories\n",
    "all_gitlab_dependencies = []\n",
    "\n",
    "for repo_data in analyzed_repos:\n",
    "    repo_dependencies = []\n",
    "    \n",
    "    # Process build files\n",
    "    for file_path, content in repo_data['build_file_contents'].items():\n",
    "        if file_path.endswith('pom.xml'):\n",
    "            deps = extract_maven_dependencies_gitlab(content)\n",
    "            repo_dependencies.extend(deps)\n",
    "        elif file_path.endswith('.gradle'):\n",
    "            deps = extract_gradle_dependencies_gitlab(content)\n",
    "            repo_dependencies.extend(deps)\n",
    "    \n",
    "    # Store dependencies for this repository\n",
    "    repo_data['dependencies'] = repo_dependencies\n",
    "    \n",
    "    # Add each dependency to the global list\n",
    "    for dep in repo_dependencies:\n",
    "        dependency_entry = {\n",
    "            'repository': repo_data['path_with_namespace'],\n",
    "            'repository_id': repo_data['id'],\n",
    "            'groupId': dep['groupId'],\n",
    "            'artifactId': dep['artifactId'],\n",
    "            'version': dep['version']\n",
    "        }\n",
    "        all_gitlab_dependencies.append(dependency_entry)\n",
    "    \n",
    "    print(f\"Found {len(repo_dependencies)} dependencies in {repo_data['name']}\")\n",
    "\n",
    "print(f\"\\nTotal GitLab dependencies found: {len(all_gitlab_dependencies)}\")\n",
    "\n",
    "# Save all dependencies\n",
    "with open(\"gitlab_dependencies.json\", \"w\") as f:\n",
    "    json.dump(all_gitlab_dependencies, f, indent=2)\n",
    "\n",
    "# Save unique dependencies\n",
    "unique_deps = set()\n",
    "for dep in all_gitlab_dependencies:\n",
    "    unique_deps.add(f\"{dep['groupId']}:{dep['artifactId']}\")\n",
    "\n",
    "unique_deps_list = sorted(list(unique_deps))\n",
    "with open(\"gitlab_unique_dependencies.json\", \"w\") as f:\n",
    "    json.dump(unique_deps_list, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(all_gitlab_dependencies)} dependencies to gitlab_dependencies.json\")\n",
    "print(f\"Saved {len(unique_deps_list)} unique dependencies to gitlab_unique_dependencies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f638afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitLab Repository Clone and Detailed Analysis\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def clone_gitlab_repo(repo_data, max_size_mb=50):\n",
    "    \"\"\"\n",
    "    Clone a GitLab repository and analyze its structure\n",
    "    \"\"\"\n",
    "    clone_url = repo_data['http_url_to_repo']\n",
    "    repo_name = repo_data['path_with_namespace']\n",
    "    \n",
    "    # Create temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        repo_dir = os.path.join(temp_dir, repo_name.replace('/', '_'))\n",
    "        \n",
    "        try:\n",
    "            # Clone with depth limit for efficiency\n",
    "            subprocess.run([\n",
    "                'git', 'clone', '--depth', '10', \n",
    "                clone_url, repo_dir\n",
    "            ], check=True, timeout=60, capture_output=True)\n",
    "            \n",
    "            # Check repository size\n",
    "            repo_size = sum(\n",
    "                os.path.getsize(os.path.join(dirpath, filename))\n",
    "                for dirpath, dirnames, filenames in os.walk(repo_dir)\n",
    "                for filename in filenames\n",
    "            ) / (1024 * 1024)  # Convert to MB\n",
    "            \n",
    "            if repo_size > max_size_mb:\n",
    "                print(f\"Skipping {repo_name}: too large ({repo_size:.1f} MB)\")\n",
    "                return None\n",
    "            \n",
    "            # Analyze repository structure\n",
    "            files_analysis = analyze_repository_structure(repo_dir)\n",
    "            \n",
    "            return {\n",
    "                'repository': repo_name,\n",
    "                'size_mb': repo_size,\n",
    "                'files': files_analysis\n",
    "            }\n",
    "            \n",
    "        except (subprocess.TimeoutExpired, subprocess.CalledProcessError) as e:\n",
    "            print(f\"Failed to clone {repo_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "def analyze_repository_structure(repo_dir):\n",
    "    \"\"\"\n",
    "    Analyze the structure of a cloned repository\n",
    "    \"\"\"\n",
    "    files_data = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(repo_dir):\n",
    "        # Skip .git directory\n",
    "        if '.git' in dirs:\n",
    "            dirs.remove('.git')\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, repo_dir)\n",
    "            \n",
    "            # Focus on Java and build files\n",
    "            if (rel_path.endswith('.java') or \n",
    "                rel_path.endswith('.xml') or \n",
    "                rel_path.endswith('.gradle') or\n",
    "                rel_path.endswith('.yml') or\n",
    "                rel_path.endswith('.yaml')):\n",
    "                \n",
    "                try:\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "                    if file_size < 100_000:  # Skip files larger than 100KB\n",
    "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content = f.read()\n",
    "                            \n",
    "                        files_data.append({\n",
    "                            'path': rel_path,\n",
    "                            'size': file_size,\n",
    "                            'lines': len(content.splitlines()),\n",
    "                            'content': content[:10000]  # Limit content to first 10KB\n",
    "                        })\n",
    "                except (UnicodeDecodeError, PermissionError):\n",
    "                    continue\n",
    "    \n",
    "    return files_data\n",
    "\n",
    "# Analyze a subset of GitLab repositories\n",
    "detailed_analysis = []\n",
    "\n",
    "for i, repo in enumerate(gitlab_java_repos[:5]):  # Analyze first 5 repositories\n",
    "    print(f\"\\nDetailed analysis {i+1}/5: {repo['path_with_namespace']}\")\n",
    "    \n",
    "    analysis = clone_gitlab_repo(repo)\n",
    "    if analysis:\n",
    "        detailed_analysis.append(analysis)\n",
    "        print(f\"  Analyzed {len(analysis['files'])} files ({analysis['size_mb']:.1f} MB)\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(2)\n",
    "\n",
    "# Save detailed analysis\n",
    "with open(\"gitlab_detailed_analysis.json\", \"w\") as f:\n",
    "    json.dump(detailed_analysis, f, indent=2)\n",
    "\n",
    "print(f\"\\nCompleted detailed analysis of {len(detailed_analysis)} repositories\")\n",
    "print(f\"Saved to gitlab_detailed_analysis.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitLab Contributors and Issues Analysis\n",
    "def get_gitlab_contributors(project_id):\n",
    "    \"\"\"\n",
    "    Get contributors for a GitLab project\n",
    "    \"\"\"\n",
    "    url = f\"{GITLAB_API_URL}/projects/{project_id}/repository/contributors\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error getting contributors for project {project_id}: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception getting contributors for project {project_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_gitlab_issues(project_id, per_page=100):\n",
    "    \"\"\"\n",
    "    Get issues for a GitLab project\n",
    "    \"\"\"\n",
    "    url = f\"{GITLAB_API_URL}/projects/{project_id}/issues\"\n",
    "    \n",
    "    params = {\n",
    "        'state': 'all',\n",
    "        'per_page': per_page,\n",
    "        'page': 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error getting issues for project {project_id}: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception getting issues for project {project_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_gitlab_commits(project_id, per_page=100):\n",
    "    \"\"\"\n",
    "    Get commits for a GitLab project\n",
    "    \"\"\"\n",
    "    url = f\"{GITLAB_API_URL}/projects/{project_id}/repository/commits\"\n",
    "    \n",
    "    params = {\n",
    "        'per_page': per_page,\n",
    "        'page': 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error getting commits for project {project_id}: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception getting commits for project {project_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Enhanced analysis with contributors, issues, and commits\n",
    "enhanced_repos = []\n",
    "\n",
    "for i, repo_data in enumerate(analyzed_repos):\n",
    "    print(f\"\\nEnhanced analysis {i+1}/{len(analyzed_repos)}: {repo_data['name']}\")\n",
    "    \n",
    "    project_id = repo_data['id']\n",
    "    \n",
    "    # Get contributors\n",
    "    contributors = get_gitlab_contributors(project_id)\n",
    "    print(f\"  Found {len(contributors)} contributors\")\n",
    "    \n",
    "    # Get issues\n",
    "    issues = get_gitlab_issues(project_id)\n",
    "    print(f\"  Found {len(issues)} issues\")\n",
    "    \n",
    "    # Get commits\n",
    "    commits = get_gitlab_commits(project_id)\n",
    "    print(f\"  Found {len(commits)} commits\")\n",
    "    \n",
    "    # Enhance repo data\n",
    "    enhanced_repo = repo_data.copy()\n",
    "    enhanced_repo['contributors'] = contributors\n",
    "    enhanced_repo['issues'] = issues\n",
    "    enhanced_repo['commits'] = commits\n",
    "    \n",
    "    enhanced_repos.append(enhanced_repo)\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save enhanced analysis\n",
    "with open(\"gitlab_enhanced_analysis.json\", \"w\") as f:\n",
    "    json.dump(enhanced_repos, f, indent=2)\n",
    "\n",
    "print(f\"\\nCompleted enhanced analysis of {len(enhanced_repos)} repositories\")\n",
    "print(f\"Saved to gitlab_enhanced_analysis.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eed30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Data Processing and Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"GitLab Scraping Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"Total GitLab repositories scraped: {len(gitlab_java_repos)}\")\n",
    "print(f\"Repositories analyzed in detail: {len(analyzed_repos)}\")\n",
    "print(f\"Total dependencies found: {len(all_gitlab_dependencies)}\")\n",
    "print(f\"Unique dependencies: {len(unique_deps_list)}\")\n",
    "\n",
    "# Group repositories by star count\n",
    "star_ranges = {\n",
    "    \"0-10 stars\": 0,\n",
    "    \"11-50 stars\": 0,\n",
    "    \"51-100 stars\": 0,\n",
    "    \"101-500 stars\": 0,\n",
    "    \"500+ stars\": 0\n",
    "}\n",
    "\n",
    "for repo in gitlab_java_repos:\n",
    "    stars = repo.get('star_count', 0)\n",
    "    if stars <= 10:\n",
    "        star_ranges[\"0-10 stars\"] += 1\n",
    "    elif stars <= 50:\n",
    "        star_ranges[\"11-50 stars\"] += 1\n",
    "    elif stars <= 100:\n",
    "        star_ranges[\"51-100 stars\"] += 1\n",
    "    elif stars <= 500:\n",
    "        star_ranges[\"101-500 stars\"] += 1\n",
    "    else:\n",
    "        star_ranges[\"500+ stars\"] += 1\n",
    "\n",
    "print(\"\\nRepository distribution by star count:\")\n",
    "for range_name, count in star_ranges.items():\n",
    "    print(f\"  {range_name}: {count}\")\n",
    "\n",
    "# Show top dependencies\n",
    "if all_gitlab_dependencies:\n",
    "    from collections import Counter\n",
    "    dep_counter = Counter()\n",
    "    for dep in all_gitlab_dependencies:\n",
    "        dep_counter[f\"{dep['groupId']}:{dep['artifactId']}\"] += 1\n",
    "    \n",
    "    print(\"\\nTop 10 most used dependencies:\")\n",
    "    for i, (dep_name, count) in enumerate(dep_counter.most_common(10), 1):\n",
    "        print(f\"  {i}. {dep_name}: {count} projects\")\n",
    "\n",
    "# Save a final summary\n",
    "summary_data = {\n",
    "    'total_repositories': len(gitlab_java_repos),\n",
    "    'analyzed_repositories': len(analyzed_repos),\n",
    "    'total_dependencies': len(all_gitlab_dependencies),\n",
    "    'unique_dependencies': len(unique_deps_list),\n",
    "    'star_distribution': star_ranges,\n",
    "    'top_dependencies': dict(dep_counter.most_common(20)) if all_gitlab_dependencies else {}\n",
    "}\n",
    "\n",
    "with open(\"gitlab_scraping_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nScraping complete! Summary saved to gitlab_scraping_summary.json\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"- gitlab_java_repos.json\")\n",
    "print(\"- gitlab_analyzed_repos.json\") \n",
    "print(\"- gitlab_dependencies.json\")\n",
    "print(\"- gitlab_unique_dependencies.json\")\n",
    "print(\"- gitlab_detailed_analysis.json\")\n",
    "print(\"- gitlab_enhanced_analysis.json\")\n",
    "print(\"- gitlab_scraping_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
